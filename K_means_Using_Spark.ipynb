{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigneshbaskar1410/Machine-Learning-Projects/blob/main/K_means_Using_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Means using Spark**"
      ],
      "metadata": {
        "id": "rRcU55v94GOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark installation using PyPI is as follows:**"
      ],
      "metadata": {
        "id": "X_D5eZkYKipS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NytKsK6T3z8h",
        "outputId": "21e3a0a6-a2b6-47b0-8876-61be5574af40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installing pydrive**"
      ],
      "metadata": {
        "id": "-TGsjjS7M8oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "id": "H8LhEby74Q4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark is written in the Scala programming language and requires Java Virtual Machine(JVM) to run. Therefore, our first task is to download Java.**"
      ],
      "metadata": {
        "id": "uEOpzsRqPS_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "id": "K4Nm3knL4WFk",
        "outputId": "5c2d0a5e-b680-4bc7-fddc-8fbbe06caffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless is already the newest version (8u342-b07-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, it is time to set the environment path. This code will enable us to run Pyspark in the colab environment.**"
      ],
      "metadata": {
        "id": "dVZ6MO9MQKzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "qAof70Bn4p86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**like we do in sklearn we are importing pandas, numpy, matplotlib here also to build the Machine Learning Model**"
      ],
      "metadata": {
        "id": "mUrUQf7ZUBfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "BJM0ImnbT6_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark                       # importing pyspark\n",
        "from pyspark.sql import *             # for working with sql   \n",
        "from pyspark.sql.types import *       #List of data types available.\n",
        "from pyspark.sql.functions import *  #List of built-in functions available for DataFrame\n",
        "from pyspark import SparkContext, SparkConf # importing SparkContext and SparkConf"
      ],
      "metadata": {
        "id": "AAd-qTRc5nbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The first thing a Spark program must do is to create a SparkContext object, which tells Spark how to access a cluster. To create a SparkContext you first need to build a SparkConf object that contains information about your application.**"
      ],
      "metadata": {
        "id": "syHtdjx6YkNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark makes heavy use of the network, and some environments have strict requirements for using tight firewall settings.\n",
        "\n",
        "\"4050\" is primary default ports that Spark uses for its communication with the help of web UI.   \n",
        "\n"
      ],
      "metadata": {
        "id": "0OILd4GaZgZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# this code tells that the information about your application contains in the sparkconf object  is setted to from web to driver of default port \"4050\".**"
      ],
      "metadata": {
        "id": "oZlPMyGScL4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the session \n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")  \n",
        " "
      ],
      "metadata": {
        "id": "EmdQQ9iD64PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*  To create a SparkContext object, which tells Spark how to access a cluster\n",
        "\n",
        "*  2nd line code, checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b6D4I-9Wdcam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "hCarxVkzdbo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Finally, printing the SparkSession Variable**"
      ],
      "metadata": {
        "id": "s9XRoRCOgB07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark   "
      ],
      "metadata": {
        "id": "_pzaS0tE74hW",
        "outputId": "ff532f53-8519-48a3-f304-3f7d9830fb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f390e6b95d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://07fab0e80481:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **This code helps to view the Spark UI (Web User Interfaces) and also able to view the jobs and their stages at the link created.**"
      ],
      "metadata": {
        "id": "mDM8-9Lrfjf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "id": "YXkA91IV9Nog",
        "outputId": "ffeec145-5b32-470f-8ec9-13c059c3b2f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-20 18:21:24--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.161.241.46, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  47.7MB/s    in 0.3s    \n",
            "\n",
            "2022-10-20 18:21:24 (47.7 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**"
      ],
      "metadata": {
        "id": "Z_pxei_j90vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the inbuild breast cancer dataset from sklearn.datasets**"
      ],
      "metadata": {
        "id": "XCGe34ySTK0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "metadata": {
        "id": "N8T7m94i90Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources.\n",
        "\n",
        "\n",
        "*   PySpark supports various UDFs and APIs to allow users to execute Python native functions.\n",
        "\n",
        "*   Resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.\n",
        "\n",
        "\n",
        "*   Create a PySpark DataFrame from an RDD consisting of a list of tuples.\n",
        "\n",
        "*   map(func) Return a new distributed dataset formed by passing each element of the source through a function.\n",
        "\n",
        "*   features - a dataframe of Dense vectors, containing all the original features in the dataset.\n",
        "\n",
        "* labels - a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer or not.\n",
        "\n",
        "* **df.printschema()** - used to print or display the schema of the DataFrame in the tree format along with column name and data type.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NKt50EMsTelK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "\n",
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable = nullable\n",
        "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
        "    return df_mod\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "In5YWJpf-D3q",
        "outputId": "cc42c9de-c00b-42a4-dc22-ee597435df47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense vectors are simply represented as NumPy array objects, so there is no need to covert them for use in MLlib. For sparse vectors, the factory methods in this class create an MLlib-compatible type, or users can pass in SciPy’s scipy.sparse column vectors."
      ],
      "metadata": {
        "id": "Lqsg8SH4lqpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row),[\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)"
      ],
      "metadata": {
        "id": "pQZxlVol_Ada"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Kmeans and ClusteringEvaluator from pyspark.ml.\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# training a K-means model\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(features)\n",
        "\n",
        "# Make Predictions\n",
        "prediction = model.transform(features)\n",
        "\n",
        "# Evaluate clustering by computing Silhoutte Score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhoutte = evaluator.evaluate(prediction)\n",
        "print('Silhoutte with squared eculidean distance = '+ str(silhoutte))"
      ],
      "metadata": {
        "id": "hoQC35A6CMqC",
        "outputId": "9c2f8e5d-15cf-44e6-f718-9dd3fe8ba451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhoutte with squared eculidean distance = 0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2LcKPlaKwq6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}